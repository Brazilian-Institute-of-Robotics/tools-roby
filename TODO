= TODO

== Renames
* remote_id => network_id
* incremental_dump? => dump_reference?
* fix Roby::Control and Roby.control. Everything should be accessible from the same
  object

== Core
* dynamic parameters. State prediction in both plan and transaction context: allow
  to predict the effect of changing the value of dynamic parameters in transaction
  context.

* a better synchronous event propagation scheme. Two parts:
    1/ create a Precedence graph in which an edge a -> b exists if b depends on
       a (signalled, forwarded, or constrainted to be "after"), then use this
       graph during event propagation. The propagation mechanism will then sort
       the current set of events according to this graph (kind of topological
       sort) and propagate only the first one, then loop until there is no more
       events to be called. Note that would properly fix all kinds of issues
       regarding 'stop' not being called last, or non-terminal events called
       *after* 'stop' while they are being called in the same cycle.
       [DONE: 20070221174851-bd67f-1e4a002da468443ff489b768c4cf5f042ce3542d.gz]
    2/ when predicates like happened? are called in a propagation context, and
       we don't know the result yet (happened? = false), register a continuation
       and its dependency on the event. Include this continuation in the event set
       and sort it accordingly. 

* in #replace, use Hierarchy#fullfilled_events to check that
  all needed events are provided. If some are missing, use define_event to try to add a new one
  dynamically

* when replacing an event generator, what to do with its handlers ?
  - either we discard them, because some handlers are used in a specific way
  - either we apply them on the new task
  - unfortunately there is no good answer... Both are useful and it is difficult
    (if not impossible) to know what to do. Having no solution for this problem
    reduces the usefulness of event handlers greatly.

* Need to tag relations with a name so that we can find what is what (kind of 'roles'). 
  For instance, in PlanningLoop, we would tag the seeds (i.e. the planned tasks) with a certain name
  which would allow to add any child without the loop code noticing

* add a multiplicity parameter for hierarchy relations which tells
  how many child task of a given kind are expected by the parent task. Add a 'realized_by'
  transition for that. For instance, in case of Pom::Localization, we can tell that the task expects
  at least one Pom::Estimator. If the last estimator breaks, we can repair the plan online by adding
  a transition.

* we NEED plan merging:
    - is we reuse a task which is already running, it should be transparent for
      the new process: this new task tree will call start!, but the task is running.
      Moreover, if it is synchronizing on the start event, it should appear "as if"
      the event has been emitted

* Check the capability we have to put every application block in transaction context. This would allow
  for instance to discard **all** plan modification done by an exception handler (or plan command, event
  handler, you get my drift) if an exception is raised. I think it would be a worthy feature
  [This can't be done because of the transaction creation cost]

* Kill local tasks that are in force_gc even if they have parents, if their parents are remote
  tasks

* rename Roby::TaskModelTag::argument into 'arguments' (beware of name clash with the
  accessor)

* Fix transaction proxying. Better use the same kind of model than in dRoby: create slightly
  modified tasks of the same model than the proxied event, do not proxy task events
* Find a way to remove the "return unless proxying?" in top of proxy_code. It sucks

== Distributed
* marshalling is too costly. We have to do it in two steps: *full* marshalling 
  is what we do now, by transfering all needed information about the object to the remote peer. It
  allows to create the object proxy on the other side. When the remote peer has initialized the
  proxy, and if we are sure that all the object updates are sent to the remote peer (in dRoby, it is
  if the object is subscribed by the remote peer), then only send a remote ID (a DRbObject)

* Use a fully asynchronous model for peer-to-peer connection, only feeding the control thread once
  in each execution cycle. This could allow to reduce peer-to-peer handshake length (which is currently
  of 3 cycles). Note however that connection should be subject to decision control, so the connection
  filtering should be done in the control thread ... Needs more thought.

* Better handling of unknown models
  - what I'd like to achieve is handling the following situation:
    peer 1 and 2 know about a SpecificModel task model
    peer 3 does not

    If 1 sees tasks of 2 *through* the plan of 3, and if these tasks are of the SpecificModel model,
    then they should be instances of the SpecificModel class 

* There is a race condition when removing objects: if a peer A is building a transaction involving
  tasks from a peer B, and if the peer B removes the tasks, then there is a race condition between
  the time B sends the transaction and the time it processes the remove_object update from A. The
  net effect is that when the transaction sibling is created, some tasks it includes do not exist
  anymore. It is more general, since the same problem exists when wrapping tasks in a transaction
  for instance.

  I don't think we should really fix the race condition per se. We should in general check if the
  object has not been removed when A processes updates from B and have some ways to feedback B from
  A.

* in display, remove transaction proxies right away: the transaction itself is removed and we're
  only interested in the resulting plan
  [DONE:20070503185040-bd67f-42a27bff79ff75e45d0a71056db88d767a4e84b8.gz]

* log snapshot: build a set of logging messages which represent the current state of the plan.
  Useful for display (it would allow to seek a lot faster) and for runtime display of the plan
  state (send the snapshot on connection)

* TCP server for display at runtime
  [DONE:20070503185040-bd67f-42a27bff79ff75e45d0a71056db88d767a4e84b8.gz]

* cleanup roby-shell: define some toplevel commands and use the <method_name>! notation to start new
  missions
  [DONE:20070430153545-bd67f-1783cb61fef4a55f2cf06963b17acdbf1ce892e5.gz]

= DONE

* fix namings in Distributed. It is quite a mess now ...
  - Peer#proxy => Peer.local_object (same for marshalled objects)
    Fix the Peer#subscriptions / PeerServer#subscriptions balance. Choose different names which are more
    descriptive
  - fix name of parameters. Use a marshalled_ prefix for marshalled objects
  [DONE: 20070208200957-bd67f-92437104352c3e5d3cda8d357066131144c0aabd.gz]

* Better handling of emit() in exception context. emit() SHALL be sent whatever exception happens. For
  instance, if an already running task calls emit() we MUST propagate that event (the timepoint has
  been reached). Nonetheless, the task will be killed by exception handling of course ;-)

* gather all events before doing any propagation. This can be simply done by calling
  event_processing in a gather_propagation context

* abstract classes. Tasks like Roby::Task or yet-to-be-planned tasks are
  abstract since they will never be executable. Think about using modules to
  define some abstract tasks. This is possible since in effect, included modules
  look like base classes. However, we want to be able to make *some* abstract
  classes instanciable, so we can't use only modules

* make all tasks non-executable until the time they are inserted in a Plan.
  Check in #fired that the parent task *is* executable
  [DONE: 20061003150012-bd67f-d897752373b665e4707ff8d45cdbabcf7fda6c59]

* Error handling
  - when a task dies, fire a ChildFailed exception. These exceptions are a **stack** of
    task that failed: if a child task fails, then we consider that the parent task
    failed as well if it cannot repair and so one. What the ChildFailed exception keeps
    is the list of failing tasks.
    If the task is killed because of the exception, we kill the higher task under
    the principle that it can have cleanup routines for its children
  - exceptions are gathered in during event propagation
    - if an exception occured in a particular event, do not fire events that should have
      been fired by this handler
  - exception handlers are defined by an exception matcher (which is tested against the
    exception object using ===).
  - if two parent branches can repair, select the one which can do that the most locally 
  - problem of multiple exceptions in the same execution cycle:
    - no task is killed before all exception have been propagated. This ensures that all
      tasks are able to handle the provided exceptions

* Implement our own marshalling mechanism for dRoby. The rules of this mechanism is to:
    - marshall everything that is marshallable
    - marshall all "global" model classes and all relations using their name and unmarshall them
      using constant(). Alternatively, for task models, send all the model ancestry so that we can
      select a more generic model if the specific model is not available

* add per-model arguments attribute for tasks, which gives the list
  of expected arguments. These lists will then be used in fullfills?  checks.  realized_by will use
  them too: if we give only a model and no arguments, initialize the argument list by taking only
  the needed arguments from the provided task.

 vim: tw=100
