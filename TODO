== TODO
* in #replace, use Hierarchy#fullfilled_events to check that
  all needed events are provided. If some are missing, use 
  define_event to try to add a new one dynamically

* when replacing an event generator, what to do with its handlers ?
  - either we discard them, because some handlers are used in a specific
    way
  - either we apply them on the new task
  - unfortunately there is no good answer... Both are useful and it is difficult
    (if not impossible) to know what to do. Having no solution for this problem
    reduces the usefulness of event handlers greatly.

* add per-model arguments attribute for tasks, which gives the list
  of expected arguments. These lists will then be used in fullfills?  checks.
  realized_by will use them too: if we give only a model and no arguments,
  initialize the argument list by taking only the needed arguments from the
  provided task.

* add a multiplicity parameter for hierarchy relations which tells
  how many child task of a given kind are expected by the parent
  task. Add a 'realized_by' transition for that. For instance, in
  case of Pom::Localization, we can tell that the task expects at
  least one Pom::Estimator. If the last estimator breaks, we can
  repair the plan online by adding a transition.

* a better synchronous event propagation based on continuations. When methods
  like happened? are called in a propagation context, and we don't know the 
  result yet (happened? = false), register a continuation and its dependency
  on the event. Call the continuation when we propagated the other branches of
  the context.

* we NEED plan merging:
    - is we reuse a task which is already running, it should be transparent for
      the new process: this new task tree will call start!, but the task is running.
      Moreover, if it is synchronizing on the start event, it should appear "as if"
      the event has been emitted

* Need to tag hierarchy relations with a name so that we can find what is what (kind of 'roles'). 
  For instance, in PlanningLoop, we would tag the seeds (i.e. the planned tasks) with a certain
  name which would allow to add any child without the loop code noticing

* Use a fully asynchronous model for peer-to-peer connection, only feeding the control thread once
  in each execution cycle. This could allow to reduce peer-to-peer handshake length (which is currently
  of 3 cycles). Note however that connection should be subject to decision control, so the connection
  filtering should be done in the control thread ... Needs more thought.

* Check that we can fix the behaviour of #happened?(true) to return false when the event is already
  in history, but from the same propagation cycle. This would need to have a new propagation ID for
  each call to #event_propagation_step instead of only #event_propagation since each step is supposed
  to represent (a false view of) the partial ordering of events.

* Check the capability we have to put every application block in transaction context. This would allow
  for instance to discard **all** plan modification done by an exception handler (or plan command, event
  handler, you get ny drift) if an exception is raised. I think it would be a worthy feature

* Implement our own marshalling mechanism for dRoby. The rules of this mechanism is to:
    - marshall everything that is marshallable
    - marshall all "global" model classes and all relations using their name
      and unmarshall them using constant(). Alternatively, for task models,
      send all the model ancestry so that we can select a more generic model if the
      specific model is not available


* fix namings in Distributed. It is quite a mess now ...
  - Peer#proxy => Peer.local_object (same for marshalled objects)
    Fix the Peer#subscriptions / PeerServer#subscriptions balance. Choose different names which are more
    descriptive
  - fix name of parameters. Use a marshalled_ prefix for marshalled objects

* Better handling of unknown models
  - what I'd like to achieve is handling the following situation:
    peer 1 and 2 know about a SpecificModel task model
    peer 3 does not

    If 1 sees tasks of 2 *through* the plan of 3, and if these tasks are of the SpecificModel model,
    then they should be instances of the SpecificModel class 

== DONE

* Better handling of emit() in exception context. emit() SHALL be sent whatever exception happens. For
  instance, if an already running task calls emit() we MUST propagate that event (the timepoint has
  been reached). Nonetheless, the task will be killed by exception handling of course ;-)

* gather all events before doing any propagation. This can be simply done by calling
  event_processing in a gather_propagation context

* abstract classes. Tasks like Roby::Task or yet-to-be-planned tasks are
  abstract since they will never be executable. Think about using modules to
  define some abstract tasks. This is possible since in effect, included modules
  look like base classes. However, we want to be able to make *some* abstract
  classes instanciable, so we can't use only modules

* make all tasks non-executable until the time they are inserted in a Plan.
  Check in #fired that the parent task *is* executable
  [DONE: 20061003150012-bd67f-d897752373b665e4707ff8d45cdbabcf7fda6c59]

* Error handling
  - when a task dies, fire a ChildFailed exception. These exceptions are a **stack** of
    task that failed: if a child task fails, then we consider that the parent task
    failed as well if it cannot repair and so one. What the ChildFailed exception keeps
    is the list of failing tasks.
    If the task is killed because of the exception, we kill the higher task under
    the principle that it can have cleanup routines for its children
  - exceptions are gathered in during event propagation
    - if an exception occured in a particular event, do not fire events that should have
      been fired by this handler
  - exception handlers are defined by an exception matcher (which is tested against the
    exception object using ===).
  - if two parent branches can repair, select the one which can do that the most locally 
  - problem of multiple exceptions in the same execution cycle:
    - no task is killed before all exception have been propagated. This ensures that all
      tasks are able to handle the provided exceptions


